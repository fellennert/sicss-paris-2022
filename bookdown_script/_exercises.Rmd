# Exercises

## Day 2

1. Start a session with the tidyverse Wikipedia page. Adapt your user agent to some sort of different value. Proceed to Hadley Wickham's page. Go back. Go forth. Check the `session_history()` to see if it has worked.

```{r eval=FALSE}
tidyverse_wiki <- "https://en.wikipedia.org/wiki/Tidyverse"
hadley_wiki <- "https://en.wikipedia.org/wiki/Hadley_Wickham"
user_agent <- httr::user_agent("Hi, I'm Felix and I try to steal your data.")
```

2. Start a session on "https://www.scrapethissite.com/pages/advanced/?gotcha=login", fill out, and submit the form. Any value for login and password will do. (Disclaimer: you have to add the URL as an "action" attribute after creating the form, see [this tutorial](https://github.com/tidyverse/rvest/issues/319). -- `login_form$action <- url`)

```{r eval=FALSE}
url <- "https://www.scrapethissite.com/pages/advanced/?gotcha=login"

#extract and set login form here

login_form$action <- url # add url as action attribute

# submit form
base_session <- session("https://www.scrapethissite.com/pages/advanced/?gotcha=login") %>% 
  session_submit(login_form) 
```

3. Scrape 10 profile timelines from the following list. Check the [documentation](https://cran.r-project.org/web/packages/rtweet/vignettes/intro.html) for instructions.

```{r eval=FALSE}
library(rtweet)

uk_mps <- lists_members(
  list_id = "217199644"
) 
```

## Day 3

1. Download all movies from the (IMDb Top250 best movies of all time list)[https://www.imdb.com/chart/top/?ref_=nv_mv_250]. Put them in a tibble with the columns `rank` -- in numeric format, `title`, `url` to IMDb entry, `rating` -- in numeric format.

```{r}
example_tbl <- tibble(
  a = 1:10,
  b = 11:20, # vectors must have the same length
  c = "c" # or length 1
)
example_tbl
```

2. Scrape the (British MPs Wikipedia page)[https://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_2019_United_Kingdom_general_election].
  a. Scrape the table containing data on all individual MPs. (Hint: you can select and remove single lines using `slice()`, e.g., `tbl %>% slice(1)` retains only the first line, `tbl %>% slice(-1)` removes only the first line.)
  b. Extract the links linking to individual pages. Make them readable using `url_absolute()` – `url_absolute("/kdfnndfkok", base = "https://wikipedia.com")` --> `"https://wikipedia.com/kdfnndfkok"`. Add the links to the table from 1a.
  c. Scrape the textual content of the **first** MP’s Wikipedia page.
  d. (ADVANCED) Wrap 2c. in a function called `scrape_mp_wiki()`.

3. Scrape the 10 first pages of [R-bloggers](https://www.r-bloggers.com) in an automated fashion. Make sure to take breaks between requests by including `Sys.sleep(2)`.
  a. Do so using the url vector we created.
  b. Do so by getting the link for the next page and a loop.
  c. Do so by using `html_session()` in a loop.

## Day 4

### Regexes

1.Write a regex for Swedish mobile number. Test it with `str_detect("+46 71-738 25 33", "insert your regex here")`. 
2. Find all Mercedes in the `mtcars` data set.
3. Take the IMDb file (`imdb <- read_csv("https://www.dropbox.com/s/81o3zzdkw737vt0/imdb2006-2016.csv?dl=1")`) and split the `Genre` column into different columns (hint: look at the `tidyr::separate()` function). How would you do it if `Genre` were a vector using `str_split_fixed()`?
4. Take this vector of heights: `heights <- c("1m30cm", "2m01cm", "3m10cm")`
    a. How can you extract the meters using the negative look behind?
    b. Bring it into numeric format (i.e., `your_solution == c(1.3, 2.01, 3.1)`) using regexes and `stringr` commands (and `as.numeric()` in the end).

### Preprocessing and analyses

1. Download the Twitter timeline data (`timelines <- read_csv(""https://www.dropbox.com/s/dpu5m3xqz4u4nv7/tweets_house_rep_party.csv?dl=1"")`. Preprocess the Tweets. 
  a. Unnest the tokens.
  b. Remove stop words. 
  c. Perform stemming. 

2. OPTIONAL: Perform the same steps, but using `spacyr`. What works better, lemmatization or stemming?

3. Count the terms per party. 
  a. Do you see party-specific differences with regard to their ten most common stemmed terms (hint: `tbl %>% slice_max(n, n = 10, with_ties = FALSE)`)? Use the following code to plot them.
```{r eval=FALSE}
 ggplot(tbl)  +
    geom_col(aes(x = n, y = reorder_within(stemmed, n, party))) +
    scale_y_reordered() +
    labs(y = "token") +
    facet_wrap(vars(party), scales = "free") 
```
  b. Is there more words you should add to your stopwords list?
  c. OPTIONAL: Do the same thing but using the spacy output and filtering only `NOUN`s and `PROPN`ouns. 
  d. Again, is there stuff to be removed? Do so using a Regex.

4. Do the same thing as in 3. but use TF-IDF instead of raw counts. How does this alter your results?

5. What else could you have done in terms of preprocessing (think of the special characters and syntax Twitter uses here)?

## Day 5

### Supervised Machine Learning

1. Measuring polarization of language through a "fake prediction." Train the same model that we trained on British MPs earlier on (which I included below for your convenience). 
First, split the new data into training and test set (`prop = 0.3` should suffice, make sure that you set `strata = party`). Train the model using the same workflow but new training data that predicts partisanship based on the Tweets' text. Predict on the test set and compare the models' accuracy. 

```{r}
set.seed(1)
timelines_gb <- read_csv("https://www.dropbox.com/s/1lrv3i655u5d7ps/timelines_gb_2022.csv?dl=1")
timelines_us <- read_csv("https://www.dropbox.com/s/iglayccyevgvume/timelines_us.csv?dl=1")

split_gb <- initial_split(timelines_gb, prop = 0.3, strata = party)
party_tweets_train_gb <- training(split_gb)
party_tweets_test_gb <- testing(split_gb)

twitter_recipe <- recipe(party ~ text, data = party_tweets_train_gb) %>% 
  step_tokenize(text) %>% # tokenize text
  themis::step_upsample(party) %>% 
  step_tokenfilter(text, max_tokens = 1000) %>%
  step_tfidf(text) 

rf_spec <- rand_forest(trees = 50) %>%
  set_engine("ranger") %>%
  set_mode("classification") 

twitter_party_rf_workflow <- workflow() %>% 
  add_recipe(twitter_recipe) %>% 
  add_model(rf_spec)

party_gb <- twitter_party_rf_workflow %>% fit(data = party_tweets_train_gb)

predictions_gb <- augment(party_gb, party_tweets_test_gb)
mean(predictions_gb$party == predictions_gb$.pred_class)
```

2. Extract Tweets from U.S. timelines that are about abortion by using the approach outlined below. You can also add your own keywords. How does the accuracy change?

```{r}
keywords <- c("abortion", "prolife", " roe ", " wade ", "roevswade", "baby", "fetus", "womb", "prochoice", "leak")
timelines_us_abortion <- timelines_us %>% 
  filter(str_detect(text, keywords %>% str_c(collapse = "|")))
```

### Topic Models

1. Check out `LDAvis` and how it orders the topics. Try to make some sense of how they are related etc. 

```{r eval=FALSE}
library(tidyverse)
library(sotu)
library(tidytext)
library(SnowballC)
library(LDAvis)

prep_lda_output <- function(dtm, lda_output){
  doc_length <- dtm %>% 
    as.matrix() %>% 
    as_tibble() %>% 
    rowwise() %>% 
    summarize(doc_sum = c_across() %>% sum()) %>% 
    pull(doc_sum)
  phi <- posterior(lda_output)$terms %>% as.matrix()
  theta <- posterior(lda_output)$topics %>% as.matrix()
  vocab <- colnames(dtm)
  term_sums <- dtm %>% 
    as.matrix() %>% 
    as_tibble() %>% 
    summarize(across(everything(), ~sum(.x))) %>% 
    as.matrix()
  svd_tsne <- function(x) tsne::tsne(svd(x)$u)
  LDAvis::createJSON(phi = phi, 
                     theta = theta,
                     vocab = vocab,
                     doc.length = doc_length,
                     term.frequency = term_sums[1,],
                     mds.method = svd_tsne
  )
}

sotu_lda_k16 <- read_rds("https://www.dropbox.com/s/vt1ctaah5gdp0xj/lda_16.rds?dl=1")

sotu_clean <- sotu_meta %>% 
  mutate(text = sotu_text %>% 
           str_replace_all("[,.]", " ")) %>% 
  filter(between(year, 1900, 2000)) %>% 
  unnest_tokens(output = token, input = text) %>% 
  anti_join(get_stopwords(), by = c("token" = "word")) %>% 
  filter(!str_detect(token, "[:digit:]")) %>% 
  mutate(token = wordStem(token, language = "en"))

sotu_dtm <- sotu_clean %>% 
  filter(str_length(token) > 1) %>% 
  count(year, token) %>% 
  group_by(token) %>% 
  filter(n() < 95) %>% # remove tokens that appear in more than 95 documents (i.e., years)
  cast_dtm(document = year, term = token, value = n)


json_lda <- prep_lda_output(sotu_dtm, sotu_lda_k16)

serVis(json_lda, out.dir = 'vis', open.browser = TRUE)

servr::daemon_stop(1)
```

2. Do the same thing for `stminsights`. The trained stm model can be downloaded `sotu_stm <- read_rds("https://www.dropbox.com/s/65bukmm42byq0dy/sotu_stm_k16.rds?dl=1")`.

```{r eval=FALSE, include=FALSE}
sotu_content_fit <- read_rds("https://www.dropbox.com/s/65bukmm42byq0dy/sotu_stm_k16.rds?dl=1")
```

```{r eval=FALSE}
library(stminsights)

out <- list(documents = prepped_docs$documents,
            vocab = prepped_docs$vocab,
            meta = prepped_docs$meta)

prepped_docs$meta$party <- as.factor(prepped_docs$meta$party)
prep <- estimateEffect(1:16 ~ party + s(year), sotu_content_fit, meta = prepped_docs$meta, uncertainty = "Global")
map(1:16, ~summary(prep, topics = .x))

save(prepped_docs, sotu_content_fit, prep, out, file = "stm_insights.RData")

run_stminsights()
```

## Day 6

No exercises.
